{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models' predictions evaluation\n",
    "\n",
    "**NOTEBOOK GOAL**: Evaluate the predicted NumberOfSales values.\n",
    "\n",
    "\n",
    "Compared predictions:\n",
    "\n",
    "- **1_RFR** - Notebook 5.3 Random forrest\n",
    "- **2_AVG** - Notebook 7.2 AVG Month ensembles\n",
    "- **3_XGB** - Notebook 6.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from import_man import *\n",
    "import collections\n",
    "\n",
    "from BIP import apply_BIP_submission_structure, apply_BIP_submission_structure_keep_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** If you cannot load the followig datasets, please go to the corresponding notebook and run it to generate the related dataset file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_53_RFR_on_prep = pd.read_csv('./dataset/test_m12_53_RFR_on_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_64_Model_XGBoost = pd.read_csv('./dataset/test_m12_64_Model_XGBoost_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_72_AVG_Month_Ensemble = pd.read_csv('./dataset/test_m12_72_AVG_Month_Ensemble.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_dict = collections.OrderedDict()\n",
    "\n",
    "# the following dataset will be evaluated\n",
    "dfs_dict['RFR'] = test_53_RFR_on_prep\n",
    "dfs_dict['XGB'] = test_64_Model_XGBoost\n",
    "dfs_dict['AVG'] = test_72_AVG_Month_Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the BIP_submission_structure_keep_actual to all the dataframes\n",
    "for mdl_lbl, df in dfs_dict.items():\n",
    "    dfs_dict[mdl_lbl] = apply_BIP_submission_structure_keep_actual(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **[EVS] Explained Variance Score**  [(reference)](http://scikit-learn.org/stable/modules/model_evaluation.html#explained-variance-score)\n",
    "\n",
    "    The mean_absolute_error function computes mean absolute error, a risk metric corresponding to the expected value of the absolute error loss or l1-norm loss.\n",
    "    \n",
    "    $$ \\texttt{explained__variance}(y, \\hat{y}) = 1 - \\frac{Var\\{ y - \\hat{y}\\}}{Var\\{y\\}} $$\n",
    "    \n",
    "    The best possible score is 1.0, lower values are worse.\n",
    "    \n",
    "2. **[MAE] Mean absolute error**  [(reference)](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error)\n",
    "\n",
    "    The mean_squared_error function computes mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.\n",
    "   \n",
    "    $$ \\text{MAE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right| $$\n",
    "    \n",
    "3. **[MSE] Mean squared error**  [(reference)](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error)\n",
    "\n",
    "    The mean_squared_error function computes mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.\n",
    "    \n",
    "    $$ \\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2 $$\n",
    "    \n",
    "4. **[RMSE] Root mean squared error**\n",
    "\n",
    "    $$ \\text{RMSE}(y, \\hat{y}) = \\sqrt{\\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2} $$\n",
    "    \n",
    "5. **[MSLE] Mean squared logarithmic error**  [(reference)](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-logarithmic-error)\n",
    "    \n",
    "    The mean_squared_log_error function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss.\n",
    "    \n",
    "    $$ \\text{MSLE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (\\log_e (1 + y_i) - \\log_e (1 + \\hat{y}_i) )^2 $$\n",
    "    \n",
    "    Where log_e (x) means the natural logarithm of x. This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate.\n",
    "\n",
    "6. **[MedAE] Median absolute error**  [(reference)](http://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error)\n",
    "\n",
    "    The median_absolute_error is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.\n",
    "    \n",
    "    $$ \\text{MedAE}(y, \\hat{y}) = \\text{median}(\\mid y_1 - \\hat{y}_1 \\mid, \\ldots, \\mid y_n - \\hat{y}_n \\mid)$$\n",
    "    \n",
    "7. **[R²] R²score, the coefficient of determination**  [(reference)](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination)\n",
    "\n",
    "    The r2_score function computes R², the coefficient of determination. It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n",
    "    \n",
    "    $$ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n_{\\text{samples}} - 1} (y_i - \\hat{y}_i)^2}{\\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\bar{y})^2}$$\n",
    "    \n",
    "    where\n",
    "    \n",
    "    $$ \\bar{y} =  \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}} - 1} y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "def print_regr_stats(df):\n",
    "    y_true = df['NumberOfSales']\n",
    "    y_pred = df['_NumberOfSales']\n",
    "    \n",
    "    stats = collections.OrderedDict()\n",
    "    stats['EVS']   = metrics.explained_variance_score(y_true, y_pred)\n",
    "    stats['MAE']   = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    stats['MSE']   = metrics.mean_squared_error(y_true, y_pred)\n",
    "    stats['RMSE']  = sqrt(stats['MSE'])\n",
    "    stats['MSLE']  = metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    stats['MedAE'] = metrics.median_absolute_error(y_true, y_pred)\n",
    "    stats['R2']    = metrics.r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(\"Explained Variance Score               [EVS] \", stats['EVS'])\n",
    "    print(\"Mean absolute error                    [MAE] \", stats['MAE'])\n",
    "    print(\"Mean squared error                     [MSE] \", stats['MSE'])\n",
    "    print(\"Root mean squared error               [RMSE] \", stats['RMSE'])\n",
    "    print(\"Mean squared logarithmic error        [MSLE] \", stats['MSLE'])\n",
    "    print(\"Median absolute error (reference)    [MedAE] \", stats['MedAE'])\n",
    "    print(\"R²score, coefficient of determination   [R²] \", stats['R2'])\n",
    "    \n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................ RFR................................\n",
      "Explained Variance Score               [EVS]  0.9516157191345931\n",
      "Mean absolute error                    [MAE]  8403.387817765266\n",
      "Mean squared error                     [MSE]  130439386.0449889\n",
      "Root mean squared error               [RMSE]  11421.006349923324\n",
      "Mean squared logarithmic error        [MSLE]  0.00879432389306739\n",
      "Median absolute error (reference)    [MedAE]  6596.779882267692\n",
      "R²score, coefficient of determination   [R²]  0.9266278413635942\n",
      "\n",
      "\n",
      "................................ XGB................................\n",
      "Explained Variance Score               [EVS]  0.9554457225986508\n",
      "Mean absolute error                    [MAE]  6066.938750547397\n",
      "Mean squared error                     [MSE]  79212072.19523914\n",
      "Root mean squared error               [RMSE]  8900.116414701502\n",
      "Mean squared logarithmic error        [MSLE]  0.005160802533279368\n",
      "Median absolute error (reference)    [MedAE]  4035.7363999999943\n",
      "R²score, coefficient of determination   [R²]  0.9554432069695348\n",
      "\n",
      "\n",
      "................................ AVG................................\n",
      "Explained Variance Score               [EVS]  0.9447316764119325\n",
      "Mean absolute error                    [MAE]  6572.52946599536\n",
      "Mean squared error                     [MSE]  102904427.66533032\n",
      "Root mean squared error               [RMSE]  10144.181961367329\n",
      "Mean squared logarithmic error        [MSLE]  0.00711851471802916\n",
      "Median absolute error (reference)    [MedAE]  4544.63999999997\n",
      "R²score, coefficient of determination   [R²]  0.9421162562935934\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATISTICS COMPARISON\n",
      "\n",
      "EVS\n",
      "[(0, ('AVG', 0.9447316764119325)),\n",
      " (1, ('RFR', 0.9516157191345931)),\n",
      " (2, ('XGB', 0.9554457225986508))]\n",
      "MAE\n",
      "[(0, ('XGB', 6066.938750547397)),\n",
      " (1, ('AVG', 6572.52946599536)),\n",
      " (2, ('RFR', 8403.387817765266))]\n",
      "MSE\n",
      "[(0, ('XGB', 79212072.19523914)),\n",
      " (1, ('AVG', 102904427.66533032)),\n",
      " (2, ('RFR', 130439386.0449889))]\n",
      "RMSE\n",
      "[(0, ('XGB', 8900.116414701502)),\n",
      " (1, ('AVG', 10144.181961367329)),\n",
      " (2, ('RFR', 11421.006349923324))]\n",
      "MSLE\n",
      "[(0, ('XGB', 0.005160802533279368)),\n",
      " (1, ('AVG', 0.00711851471802916)),\n",
      " (2, ('RFR', 0.00879432389306739))]\n",
      "MedAE\n",
      "[(0, ('XGB', 4035.7363999999943)),\n",
      " (1, ('AVG', 4544.63999999997)),\n",
      " (2, ('RFR', 6596.779882267692))]\n",
      "R2\n",
      "[(0, ('RFR', 0.9266278413635942)),\n",
      " (1, ('AVG', 0.9421162562935934)),\n",
      " (2, ('XGB', 0.9554432069695348))]\n"
     ]
    }
   ],
   "source": [
    "models_stats = collections.OrderedDict()\n",
    "models_stats['EVS']   = []\n",
    "models_stats['MAE']   = []\n",
    "models_stats['MSE']   = []\n",
    "models_stats['RMSE']  = []\n",
    "models_stats['MSLE']  = []\n",
    "models_stats['MedAE'] = []\n",
    "models_stats['R2']    = []\n",
    "\n",
    "\n",
    "for mdl_lbl, df in dfs_dict.items():\n",
    "    print('................................ ' + mdl_lbl + '................................')\n",
    "    stats = print_regr_stats(df)\n",
    "    \n",
    "    # add the model statistics to the main index\n",
    "    for name, val in stats.items():\n",
    "        models_stats[name].append((mdl_lbl, val))\n",
    "        \n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "print(\"\\n\\nSTATISTICS COMPARISON\\n\")\n",
    "\n",
    "for name, l in models_stats.items():\n",
    "    print(name)\n",
    "    pprint(list(enumerate(sorted(l, key=lambda tup: tup[1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
